{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Event Lip Reading\n",
    "\n",
    "**Author**: Adly Zaroui\n",
    "\n",
    "The \"Event Lip Reading\" project aims to develop a system that can recognize and interpret lip movements from event-based sensor data. Event-based sensors are capable of capturing high-frequency changes in luminosity, making them ideal for applications that require a high refresh rate. In this project, the event data consists of tuples containing the pixel coordinates, polarity (increase or decrease in luminosity), and timestamp of each event.\n",
    "\n",
    "The goal of the project is to classify these events and extract meaningful information from them. This can be achieved by aggregating the events into superframes, converting them into images, and applying various image processing techniques. The project involves tasks such as loading the event data, converting events to images, creating animations from the images, and aggregating events into superframes.\n",
    "\n",
    "By analyzing the lip movements captured by the event-based sensor, the system can potentially be used for lip reading applications, speech recognition, and other related tasks. The project utilizes Python, pandas, numpy, matplotlib, and other libraries to process and visualize the event data.\n",
    "\n",
    "To analyze event data, there is mainly 2 ways: the first one is to convert the events into conventional video data (sequence of images - sequence of matrices of pixels values) then apply conventional video classification algorithms. The first path that we will dive into is to treat event data as they are: multivariate time series, then explore classification techniques on mulitvariate time series.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's define a function that loads the data and then use it to load he data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:10<01:31, 10.15s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/adly/Documents/EventLipReading/temporary.ipynb Cell 3\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adly/Documents/EventLipReading/temporary.ipynb#W2sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m class_folders \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mAddition\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCarnaval\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mDecider\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mEcole\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mFillette\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mHuitre\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mJoyeux\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMusique\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mPyjama\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mRuisseau\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adly/Documents/EventLipReading/temporary.ipynb#W2sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m \u001b[39m# Load the training and test data\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/adly/Documents/EventLipReading/temporary.ipynb#W2sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m x_train \u001b[39m=\u001b[39m load_data(train_base_path, class_folders)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adly/Documents/EventLipReading/temporary.ipynb#W2sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39m#x_test = load_data(test_base_path)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adly/Documents/EventLipReading/temporary.ipynb#W2sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adly/Documents/EventLipReading/temporary.ipynb#W2sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39m# Create target data\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adly/Documents/EventLipReading/temporary.ipynb#W2sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m y_train \u001b[39m=\u001b[39m []\n",
      "\u001b[1;32m/Users/adly/Documents/EventLipReading/temporary.ipynb Cell 3\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adly/Documents/EventLipReading/temporary.ipynb#W2sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m csv_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(base_path, folder, csv_file)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adly/Documents/EventLipReading/temporary.ipynb#W2sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39m# Load the CSV file into a dataframe\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/adly/Documents/EventLipReading/temporary.ipynb#W2sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(csv_path)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adly/Documents/EventLipReading/temporary.ipynb#W2sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39m# Append the dataframe to the list\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adly/Documents/EventLipReading/temporary.ipynb#W2sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m dataframes[folder]\u001b[39m.\u001b[39mappend(df)\n",
      "File \u001b[0;32m~/Documents/EventLipReading/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/Documents/EventLipReading/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:617\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    614\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[1;32m    616\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[0;32m--> 617\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
      "File \u001b[0;32m~/Documents/EventLipReading/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1748\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1741\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[1;32m   1742\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1743\u001b[0m     \u001b[39m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m     (\n\u001b[1;32m   1745\u001b[0m         index,\n\u001b[1;32m   1746\u001b[0m         columns,\n\u001b[1;32m   1747\u001b[0m         col_dict,\n\u001b[0;32m-> 1748\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(  \u001b[39m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1749\u001b[0m         nrows\n\u001b[1;32m   1750\u001b[0m     )\n\u001b[1;32m   1751\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   1752\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/Documents/EventLipReading/.venv/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49mread_low_memory(nrows)\n\u001b[1;32m    235\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:843\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:904\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:879\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2050\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m<frozen codecs>:331\u001b[0m, in \u001b[0;36mgetstate\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm # for tracking progress\n",
    "\n",
    "def load_data(base_path, class_folders=None):\n",
    "    '''\n",
    "    \n",
    "    Loads the data from the specified base path.\n",
    "    \n",
    "    Parameters:\n",
    "        base_path (str): The base path to the data.\n",
    "        class_folders (list): The list of class folders names to load.\n",
    "        \n",
    "    Returns:\n",
    "        dataframes (dict): A dictionary of dataframes.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Initialize a dictionary to hold dataframes\n",
    "    dataframes = {}\n",
    "\n",
    "    if class_folders:\n",
    "        # Loop over each class folder\n",
    "        for folder in tqdm(class_folders):\n",
    "            # Initialize a list to hold dataframes for this class\n",
    "            dataframes[folder] = []\n",
    "            \n",
    "            # Get the list of CSV files in this class folder\n",
    "            csv_files = os.listdir(os.path.join(base_path, folder))\n",
    "            \n",
    "            # Loop over each CSV file\n",
    "            for csv_file in csv_files:\n",
    "                # Define the full path to the CSV file\n",
    "                csv_path = os.path.join(base_path, folder, csv_file)\n",
    "                \n",
    "                # Load the CSV file into a dataframe\n",
    "                df = pd.read_csv(csv_path)\n",
    "                \n",
    "                # Append the dataframe to the list\n",
    "                dataframes[folder].append(df)\n",
    "    else:\n",
    "        # Get the list of CSV files in the base path\n",
    "        csv_files = os.listdir(base_path)\n",
    "\n",
    "        # Loop over each CSV file\n",
    "        for csv_file in tqdm(csv_files):\n",
    "            # Define the full path to the CSV file\n",
    "            csv_path = os.path.join(base_path, csv_file)\n",
    "            \n",
    "            # Load the CSV file into a dataframe\n",
    "            df = pd.read_csv(csv_path)\n",
    "            \n",
    "            # Append the dataframe to the list\n",
    "            dataframes.append(df)\n",
    "\n",
    "    return dataframes\n",
    "\n",
    "# Define the base paths\n",
    "train_base_path = 'train10/train10/'\n",
    "test_base_path = '/test10/test10/'\n",
    "\n",
    "# Define the class folders\n",
    "class_folders = ['Addition', 'Carnaval', 'Decider', 'Ecole', 'Fillette', 'Huitre', 'Joyeux', 'Musique', 'Pyjama', 'Ruisseau']\n",
    "\n",
    "# Load the training and test data\n",
    "x_train = load_data(train_base_path, class_folders)\n",
    "#x_test = load_data(test_base_path)\n",
    "\n",
    "# Create target data\n",
    "y_train = []\n",
    "for i, class_name in enumerate(class_folders):\n",
    "    y_train.extend([i] * len(x_train[class_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "Successfully installed tqdm-4.66.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "***\n",
    "Now let's define a function to plot an image from an event dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def events_to_image(df, x_max=640, y_max=480):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Converts a dataframe of events to a 2D image taht is a 2D histogram of the events.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): A dataframe of events.\n",
    "        x_max (int): The maximum x coordinate, defaults to 100.\n",
    "        y_max (int): The maximum y coordinate, defaults to 100.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: A 2D image.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    from scipy.ndimage import rotate\n",
    "    \n",
    "    # Create a 2D histogram of the event data\n",
    "    hist, _, _ = np.histogram2d(df['x'], df['y'], bins=[x_max, y_max], weights=df['polarity'])\n",
    "\n",
    "    # Normalize the histogram to the range [0, 255]\n",
    "    hist = 255 * (hist - np.min(hist)) / (np.max(hist) - np.min(hist))\n",
    "    \n",
    "    hist = rotate(hist, 225)\n",
    "\n",
    "    return hist.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "test_event_df = train_data['Addition'][0]\n",
    "test_images = events_to_image(test_event_df)\n",
    "\n",
    "# Display an example image\n",
    "plt.imshow(test_images, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Now let's define some functions convert an event dataframe into a sequence of images, incrisignly *w.r.t.* timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_agg(x, y, polarity, timestamp, T_r=100000, M=640, N=480):\n",
    "    '''\n",
    "    Aggregate events into superframes.\n",
    "    \n",
    "    Args:\n",
    "        x (np.array): x coordinates of events\n",
    "        y (np.array): y coordinates of events\n",
    "        polarity (np.array): polarity of events\n",
    "        timestamp (np.array): timestamp of events\n",
    "        T_r (float): time interval of superframes, defaults to 100000.\n",
    "        M (int): image length, defaults to 640.\n",
    "        N (int): image width, defaults to 480.\n",
    "        \n",
    "    Returns:\n",
    "        superframes (np.array): superframes\n",
    "    '''\n",
    "    \n",
    "    from scipy.ndimage import rotate\n",
    "    \n",
    "    T_seq = timestamp.max()\n",
    "    T_frames = int((T_seq // T_r)) + 1\n",
    "    \n",
    "    frames_0 = np.zeros((T_frames, M, N)) # polarity == 0\n",
    "    frames_1 = np.zeros((T_frames, M, N)) # polarity == 1\n",
    "    \n",
    "    for i in tqdm(range(T_frames)):\n",
    "        idx_0 = np.where((timestamp >= i * T_r) & (timestamp < (i+1) * T_r) & (polarity == 0))[0]\n",
    "        if len(idx_0) > 0:\n",
    "            frames_0[i] = np.bincount(N * x[idx_0] + y[idx_0], minlength = M * N).reshape(M, N)\n",
    "        \n",
    "        idx_1 = np.where((timestamp >= i * T_r) & (timestamp < (i+1) * T_r) & (polarity == 1))[0]\n",
    "        if len(idx_1) > 0:\n",
    "            frames_1[i] = np.bincount(N * x[idx_1] + y[idx_1], minlength = M * N).reshape(M, N)\n",
    "    \n",
    "    # Rotate the frames\n",
    "    frames_0 = rotate(frames_0, 225, axes=(1,2), reshape=False)\n",
    "    frames_1 = rotate(frames_1, 225, axes=(1,2), reshape=False)\n",
    "    \n",
    "    superframes = np.concatenate((frames_0, frames_1), axis = 0)\n",
    "    print('generated superframes with size:', superframes.shape)\n",
    "    return superframes\n",
    "    \n",
    "def decompose_events(test_data):\n",
    "    '''\n",
    "    \n",
    "    Decompose the events dataframe into tuple of individual columns x, y, polarity, and timestamp.\n",
    "    \n",
    "    Args:\n",
    "        test_data (pd.DataFrame): A dataframe of events.\n",
    "        \n",
    "    Returns:\n",
    "        4-tuple: x (np.array): x coordinates of events,\n",
    "                 y (np.array): y coordinates of events,\n",
    "                 polarity (np.array): polarity of events,\n",
    "                 timestamp (np.array): timestamp of events  \n",
    "    '''\n",
    "    \n",
    "    return (np.array(test_data['x'].values),\n",
    "            np.array(test_data['y'].values),\n",
    "            np.array(test_data['polarity'].values),\n",
    "            np.array(test_data['time'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Define the superframes '''\n",
    "\n",
    "test_event_df = train_data['Addition'][0]\n",
    "\n",
    "x, y, polarity, timestamp = decompose_events(test_event_df)\n",
    "\n",
    "superframes = event_agg(x, y, polarity, timestamp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Watch the superframes '''\n",
    "    \n",
    "from matplotlib import cm\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "    \n",
    "frames = [] # for storing the generated images\n",
    "fig = plt.figure()\n",
    "for i in range(superframes.shape[0]):\n",
    "    frames.append([plt.imshow(superframes[i], cmap = cm.Greys_r, animated = True)])\n",
    "\n",
    "ani = animation.ArtistAnimation(fig, frames, interval = 50, blit = False, repeat_delay = 1000)\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "window_size = 5 # window size for smoothing\n",
    "# Denoise and compute statistics for each DataFrame\n",
    "flat_agg_x_train = []\n",
    "for key in x_train:\n",
    "    for event_df in x_train[key]:\n",
    "        event_df_smooth = event_df.rolling(window_size).mean().dropna()\n",
    "\n",
    "        stats = event_df_smooth.agg(['mean', 'std', 'max', 'min', 'skew', 'kurt', 'median', 'sem'])\n",
    "        flat_agg_x_train.append(stats.values.flatten())\n",
    "        \n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(flat_agg_x_train)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=0.95)  # Keep 95% of variance\n",
    "pca_x_train = pca.fit_transform(x_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "# Create a scatter plot of the first two principal components\n",
    "scatter = plt.scatter(pca_x_train[:, 0], pca_x_train[:, 1], c=y_train, cmap=cm.Set1)\n",
    "\n",
    "# Add a colorbar\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_label('Classes')\n",
    "\n",
    "# Add a title and labels\n",
    "plt.title('PCA of x_train')\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
